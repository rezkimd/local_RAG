{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a688841",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation using Gemma LLMs\n",
    "\n",
    "Deskripsi :\n",
    "Projek ini ditujukan untuk mengimplementasikan Retrieval-Augmented Generation secara lokal baik sistem serta databasenya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e468d8b",
   "metadata": {},
   "source": [
    "## Key terms\n",
    "\n",
    "| Term                                | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
    "| ----------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Token**                           | A sub-word piece of text. For example, \"hello, world!\" could be split into [\"hello\", \",\", \"world\", \"!\"]. A token can be a whole word,<br> part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.<br> Text gets broken into tokens before being passed to an LLM.                                                                                                                                                                                                                                                                                  |\n",
    "| **Embedding**                       | A learned numerical representation of a piece of data. For example, a sentence of text could be represented by a vector with<br> 768 values. Similar pieces of text (in meaning) will ideally have similar values.                                                                                                                                                                                                                                                                                                                                                                                        |\n",
    "| **Embedding model**                 | A model designed to accept input data and output a numerical representation. For example, a text embedding model may take in 384 <br>tokens of text and turn it into a vector of size 768. An embedding model can and often is different to an LLM model.                                                                                                                                                                                                                                                                                                                                                 |\n",
    "| **Similarity search/vector search** | Similarity search/vector search aims to find two vectors which are close together in high-demensional space. For example, <br>two pieces of similar text passed through an embedding model should have a high similarity score, whereas two pieces of text about<br> different topics will have a lower similarity score. Common similarity score measures are dot product and cosine similarity.                                                                                                                                                                                                         |\n",
    "| **Large Language Model (LLM)**      | A model which has been trained to numerically represent the patterns in text. A generative LLM will continue a sequence when given a sequence. <br>For example, given a sequence of the text \"hello, world!\", a genertive LLM may produce \"we're going to build a RAG pipeline today!\".<br> This generation will be highly dependant on the training data and prompt.                                                                                                                                                                                                                                     |\n",
    "| **LLM context window**              | The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default context window of 32k tokens<br> (about 96 pages of text) but can go up to 128k if needed. A recent open-source LLM from Google, Gemma (March 2024) has a context<br> window of 8,192 tokens (about 24 pages of text). A higher context window means an LLM can accept more relevant information<br> to assist with a query. For example, in a RAG pipeline, if a model has a larger context window, it can accept more reference items<br> from the retrieval system to aid with its generation.      |\n",
    "| **Prompt**                          | A common term for describing the input to a generative LLM. The idea of \"[prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)\" is to structure a text-based<br> (or potentially image-based as well) input to a generative LLM in a specific way so that the generated output is ideal. This technique is<br> possible because of a LLMs capacity for in-context learning, as in, it is able to use its representation of language to breakdown <br>the prompt and recognize what a suitable output may be (note: the output of LLMs is probable, so terms like \"may output\" are used). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c47ec",
   "metadata": {},
   "source": [
    "## Requirements and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13480dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_from_requirements(requirements_file=\"requirements.txt\"):\n",
    "    try:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_file]\n",
    "        )\n",
    "        print(f\"Sukses menginstal semua paket dari {requirements_file}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Gagal menginstal paket dari {requirements_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3e773",
   "metadata": {},
   "source": [
    "## 1. PDF Document Reading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f9be410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'human-nutrition-text.pdf' sudah ada di 'information_file\\human-nutrition-text.pdf'.\n",
      "Pemeriksaan: File 'human-nutrition-text.pdf' ditemukan di dalam folder 'information_file'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Nama folder tujuan\n",
    "target_folder = \"information_file\"\n",
    "\n",
    "# Nama file PDF\n",
    "pdf_filename = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Path lengkap file PDF di dalam folder target\n",
    "pdf_path = os.path.join(target_folder, pdf_filename)\n",
    "\n",
    "# URL file PDF yang ingin diunduh\n",
    "url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "# Buat folder jika belum ada\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)\n",
    "    print(f\"Folder '{target_folder}' telah dibuat.\")\n",
    "\n",
    "# Download PDF jika belum ada\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"File '{pdf_filename}' tidak ditemukan di '{target_folder}'. Mengunduh...\")\n",
    "\n",
    "    try:\n",
    "        # Kirim permintaan GET ke URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Akan memunculkan HTTPError untuk respons yang buruk (status code 4xx atau 5xx)\n",
    "\n",
    "        # Buka file dalam mode binary write dan simpan konten\n",
    "        with open(pdf_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File '{pdf_filename}' telah diunduh dan disimpan di '{pdf_path}'.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengunduh file. Error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"File '{pdf_filename}' sudah ada di '{pdf_path}'.\")\n",
    "\n",
    "# Cek keberadaan file setelah (mencoba) diunduh\n",
    "if os.path.exists(pdf_path):\n",
    "    print(f\"Pemeriksaan: File '{pdf_filename}' ditemukan di dalam folder '{target_folder}'.\")\n",
    "else:\n",
    "    print(f\"Pemeriksaan: File '{pdf_filename}' TIDAK ditemukan di dalam folder '{target_folder}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b0bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rezki\\anaconda3\\envs\\local_rag_gemma_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "1208it [00:01, 920.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': -41,\n",
       "  'page_char_count': 29,\n",
       "  'page_word_count': 4,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 7.25,\n",
       "  'text': 'Human Nutrition: 2020 Edition'},\n",
       " {'page_number': -40,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires !pip install PyMuPDF, see: https://github.com/pymupdf/pymupdf\n",
    "import pymupdf # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
    "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm \n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
    "\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    #doc = fitz.open(pdf_path)  # open a document\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49093be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.00</td>\n",
       "      <td>198.30</td>\n",
       "      <td>9.97</td>\n",
       "      <td>287.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.38</td>\n",
       "      <td>95.76</td>\n",
       "      <td>6.19</td>\n",
       "      <td>140.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.00</td>\n",
       "      <td>134.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>190.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1231.50</td>\n",
       "      <td>214.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>307.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1603.50</td>\n",
       "      <td>271.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>400.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>577.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1208.00          1208.00          1208.00                  1208.00   \n",
       "mean        562.50          1148.00           198.30                     9.97   \n",
       "std         348.86           560.38            95.76                     6.19   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%         260.75           762.00           134.00                     4.00   \n",
       "50%         562.50          1231.50           214.50                    10.00   \n",
       "75%         864.25          1603.50           271.00                    14.00   \n",
       "max        1166.00          2308.00           429.00                    32.00   \n",
       "\n",
       "       page_token_count  \n",
       "count           1208.00  \n",
       "mean             287.00  \n",
       "std              140.10  \n",
       "min                0.00  \n",
       "25%              190.50  \n",
       "50%              307.88  \n",
       "75%              400.88  \n",
       "max              577.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taking sample of text from the information file\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "random.sample(pages_and_texts, k=3)\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()\n",
    "\n",
    "#stats of the book we collected\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e8667",
   "metadata": {},
   "source": [
    "## 2. Text Splitting/Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac15b1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This another sentence.]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/ \n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a document instance as an example\n",
    "doc = nlp(\"This is a sentence. This another sentence.\")\n",
    "assert len(list(doc.sents)) == 2\n",
    "\n",
    "# Access the sentences of the document\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa91ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [00:02<00:00, 524.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    \n",
    "    # Make sure all sentences are strings\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    \n",
    "    # Count the sentences \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9039df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 985,\n",
       "  'page_char_count': 67,\n",
       "  'page_word_count': 15,\n",
       "  'page_sentence_count_raw': 3,\n",
       "  'page_token_count': 16.75,\n",
       "  'text': 'PART\\xa0XVII  CHAPTER 17. FOOD SAFETY  Chapter 17. Food Safety  |  985',\n",
       "  'sentences': ['PART\\xa0XVII  CHAPTER 17.',\n",
       "   'FOOD SAFETY  Chapter 17.',\n",
       "   'Food Safety  |  985'],\n",
       "  'page_sentence_count_spacy': 3}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an example\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d1bd0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [00:00<00:00, 403626.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10 \n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bdd00ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 74,\n",
       "  'page_char_count': 2158,\n",
       "  'page_word_count': 378,\n",
       "  'page_sentence_count_raw': 17,\n",
       "  'page_token_count': 539.5,\n",
       "  'text': 'From the Stomach to the Small Intestine  When food enters the stomach, a highly muscular organ, powerful  peristaltic contractions help mash, pulverize, and churn food into  chyme. Chyme is a semiliquid mass of partially digested food that  also contains gastric juices secreted by cells in the stomach. These  gastric juices contain hydrochloric acid and the enzyme pepsin, that  chemically start breakdown of the protein components of food.  The length of time food spends in the stomach varies by the  macronutrient composition of the meal. A high-fat or high-protein  meal takes longer to break down than one rich in carbohydrates.  It usually takes a few hours after a meal to empty the stomach  contents completely into the small intestine.  The small intestine is divided into three structural parts: the  duodenum, the jejunum, and the ileum. Once the chyme enters the  duodenum (the first segment of the small intestine), the pancreas  and gallbladder are stimulated and release juices that aid in  digestion. The pancreas secretes up to 1.5 liters (.4 US gallons) of  pancreatic juice through a duct into the duodenum per day. This  fluid consists mostly of water, but it also contains bicarbonate ions  that neutralize the acidity of the stomach-derived chyme and  enzymes that further break down proteins, carbohydrates, and  lipids. The gallbladder secretes a much smaller amount of a fluid  called bile that helps to digest fats. Bile passes through a duct that  joins the pancreatic ducts and is released into the duodenum. Bile  is made in the liver and stored in the gall bladder. Bile’s components  act like detergents by surrounding fats similar to the way dish soap  removes grease from a frying pan. This allows for the movement of  fats in the watery environment of the small intestine. Two different  types of muscular contractions, called peristalsis and segmentation,  control the movement and mixing of the food in various stages of  digestion through the small intestine.  Similar to what occurs in the esophagus and stomach, peristalsis  is circular waves of smooth muscle contraction that propel food  74  |  The Digestive System',\n",
       "  'sentences': ['From the Stomach to the Small Intestine  When food enters the stomach, a highly muscular organ, powerful  peristaltic contractions help mash, pulverize, and churn food into  chyme.',\n",
       "   'Chyme is a semiliquid mass of partially digested food that  also contains gastric juices secreted by cells in the stomach.',\n",
       "   'These  gastric juices contain hydrochloric acid and the enzyme pepsin, that  chemically start breakdown of the protein components of food.',\n",
       "   ' The length of time food spends in the stomach varies by the  macronutrient composition of the meal.',\n",
       "   'A high-fat or high-protein  meal takes longer to break down than one rich in carbohydrates.',\n",
       "   ' It usually takes a few hours after a meal to empty the stomach  contents completely into the small intestine.',\n",
       "   ' The small intestine is divided into three structural parts: the  duodenum, the jejunum, and the ileum.',\n",
       "   'Once the chyme enters the  duodenum (the first segment of the small intestine), the pancreas  and gallbladder are stimulated and release juices that aid in  digestion.',\n",
       "   'The pancreas secretes up to 1.5 liters (.4 US gallons) of  pancreatic juice through a duct into the duodenum per day.',\n",
       "   'This  fluid consists mostly of water, but it also contains bicarbonate ions  that neutralize the acidity of the stomach-derived chyme and  enzymes that further break down proteins, carbohydrates, and  lipids.',\n",
       "   'The gallbladder secretes a much smaller amount of a fluid  called bile that helps to digest fats.',\n",
       "   'Bile passes through a duct that  joins the pancreatic ducts and is released into the duodenum.',\n",
       "   'Bile  is made in the liver and stored in the gall bladder.',\n",
       "   'Bile’s components  act like detergents by surrounding fats similar to the way dish soap  removes grease from a frying pan.',\n",
       "   'This allows for the movement of  fats in the watery environment of the small intestine.',\n",
       "   'Two different  types of muscular contractions, called peristalsis and segmentation,  control the movement and mixing of the food in various stages of  digestion through the small intestine.',\n",
       "   ' Similar to what occurs in the esophagus and stomach, peristalsis  is circular waves of smooth muscle contraction that propel food  74  |  The Digestive System'],\n",
       "  'page_sentence_count_spacy': 17,\n",
       "  'sentence_chunks': [['From the Stomach to the Small Intestine  When food enters the stomach, a highly muscular organ, powerful  peristaltic contractions help mash, pulverize, and churn food into  chyme.',\n",
       "    'Chyme is a semiliquid mass of partially digested food that  also contains gastric juices secreted by cells in the stomach.',\n",
       "    'These  gastric juices contain hydrochloric acid and the enzyme pepsin, that  chemically start breakdown of the protein components of food.',\n",
       "    ' The length of time food spends in the stomach varies by the  macronutrient composition of the meal.',\n",
       "    'A high-fat or high-protein  meal takes longer to break down than one rich in carbohydrates.',\n",
       "    ' It usually takes a few hours after a meal to empty the stomach  contents completely into the small intestine.',\n",
       "    ' The small intestine is divided into three structural parts: the  duodenum, the jejunum, and the ileum.',\n",
       "    'Once the chyme enters the  duodenum (the first segment of the small intestine), the pancreas  and gallbladder are stimulated and release juices that aid in  digestion.',\n",
       "    'The pancreas secretes up to 1.5 liters (.4 US gallons) of  pancreatic juice through a duct into the duodenum per day.',\n",
       "    'This  fluid consists mostly of water, but it also contains bicarbonate ions  that neutralize the acidity of the stomach-derived chyme and  enzymes that further break down proteins, carbohydrates, and  lipids.',\n",
       "    'The gallbladder secretes a much smaller amount of a fluid  called bile that helps to digest fats.',\n",
       "    'Bile passes through a duct that  joins the pancreatic ducts and is released into the duodenum.',\n",
       "    'Bile  is made in the liver and stored in the gall bladder.',\n",
       "    'Bile’s components  act like detergents by surrounding fats similar to the way dish soap  removes grease from a frying pan.',\n",
       "    'This allows for the movement of  fats in the watery environment of the small intestine.',\n",
       "    'Two different  types of muscular contractions, called peristalsis and segmentation,  control the movement and mixing of the food in various stages of  digestion through the small intestine.',\n",
       "    ' Similar to what occurs in the esophagus and stomach, peristalsis  is circular waves of smooth muscle contraction that propel food  74  |  The Digestive System']],\n",
       "  'num_chunks': 1}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e6edd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [00:00<00:00, 27726.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1843"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992c8f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>583.38</td>\n",
       "      <td>734.44</td>\n",
       "      <td>112.33</td>\n",
       "      <td>183.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>347.79</td>\n",
       "      <td>447.54</td>\n",
       "      <td>71.22</td>\n",
       "      <td>111.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>280.50</td>\n",
       "      <td>315.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.00</td>\n",
       "      <td>746.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>186.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>890.00</td>\n",
       "      <td>1118.50</td>\n",
       "      <td>173.00</td>\n",
       "      <td>279.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>1831.00</td>\n",
       "      <td>297.00</td>\n",
       "      <td>457.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count      1843.00           1843.00           1843.00            1843.00\n",
       "mean        583.38            734.44            112.33             183.61\n",
       "std         347.79            447.54             71.22             111.89\n",
       "min         -41.00             12.00              3.00               3.00\n",
       "25%         280.50            315.00             44.00              78.75\n",
       "50%         586.00            746.00            114.00             186.50\n",
       "75%         890.00           1118.50            173.00             279.62\n",
       "max        1166.00           1831.00            297.00             457.75"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats about our chunks\n",
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2e24d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 11.0 | Text: 978 | Food Supplements and Food Replacements\n",
      "Chunk token count: 5.25 | Text: Young Adulthood | 907\n",
      "Chunk token count: 24.25 | Text: There are several lecithin supplements on the market Nonessential and Essential Fatty Acids | 315\n",
      "Chunk token count: 20.5 | Text: PART XVI CHAPTER 16. PERFORMANCE NUTRITION Chapter 16. Performance Nutrition | 931\n",
      "Chunk token count: 16.25 | Text: Table 14.2  Micronutrient Levels during Puberty 886 | Adolescence\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_length = 30\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1596f56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': -39,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FIALKOWSKI REVILLA, JENNIFER DRAPER, GEMADY LANGFELDER, CHERYL GIBBY, CHYNA NICOLE CHUN, AND ALLISON CALABRESE',\n",
       "  'chunk_char_count': 308,\n",
       "  'chunk_word_count': 42,\n",
       "  'chunk_token_count': 77.0},\n",
       " {'page_number': -38,\n",
       "  'sentence_chunk': 'Human Nutrition: 2020 Edition by University of Hawai‘i at Mānoa Food Science and Human Nutrition Program is licensed under a Creative Commons Attribution 4.0 International License, except where otherwise noted.',\n",
       "  'chunk_char_count': 210,\n",
       "  'chunk_word_count': 30,\n",
       "  'chunk_token_count': 52.5}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks_over_min_token_len = df[\n",
    "    df[\"chunk_token_count\"] > min_token_length\n",
    "].to_dict(orient=\"records\")\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09610060",
   "metadata": {},
   "source": [
    "## 3. Chunk Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049653b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires: !pip install transformers torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "embedding_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "embedding_model.to(device)\n",
    "embedding_model.eval()  # set model to evaluation mode\n",
    "\n",
    "# List of sentences\n",
    "sentences = [\n",
    "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
    "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
    "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
    "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode each sentence and get the embedding\n",
    "embeddings_dict = {}\n",
    "for sentence in sentences:\n",
    "    # Tokenize and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():  # no need to calculate gradients\n",
    "        outputs = embedding_model(**inputs)\n",
    "\n",
    "    # Use the [CLS] token representation as the sentence embedding\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    \n",
    "    embeddings_dict[sentence] = cls_embedding\n",
    "\n",
    "# Print embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01cd748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Yo! How cool are embeddings?\n",
      "Token IDs:\n",
      "tensor([[  101, 10930,   999,  2129,  4658,  2024,  7861,  8270,  4667,  2015,\n",
      "          1029,   102]])\n",
      "Token IDs shape: torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "single_sentence = \"Yo! How cool are embeddings?\"\n",
    "# Gunakan tokenizer() bukan encode()\n",
    "\n",
    "inputs = tokenizer(single_sentence, return_tensors='pt')\n",
    "\n",
    "print(f\"Sentence: {single_sentence}\")\n",
    "print(f\"Token IDs:\\n{inputs['input_ids']}\")\n",
    "print(f\"Token IDs shape: {inputs['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d0d814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1680/1680 [05:44<00:00,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13min 43s\n",
      "Wall time: 5min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Proses Embedding\n",
    "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "    sentence = item[\"sentence_chunk\"]\n",
    "    \n",
    "    # Tokenisasi + pindah ke CPU\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "\n",
    "    # Ambil embedding dari token [CLS]\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    \n",
    "    # Simpan ke dictionary\n",
    "    item[\"embedding\"] = cls_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh output\n",
    "for item in pages_and_chunks_over_min_token_len:\n",
    "    print(\"Sentence:\", item[\"sentence_chunk\"])\n",
    "    print(\"Embedding:\", item[\"embedding\"][:5], \"...\")  # Cetak 5 dimensi awal\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "375ed7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil text_chunks dari data\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "\n",
    "# Fungsi untuk batching dan embedding\n",
    "def get_bert_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenisasi batch\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**inputs)\n",
    "\n",
    "        # Ambil CLS token embeddings\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # shape: (batch_size, hidden_size)\n",
    "        embeddings.append(cls_embeddings.cpu())  # pindahkan ke CPU agar aman\n",
    "\n",
    "    # Gabungkan semua batch\n",
    "    return torch.cat(embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3d4e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [07:30<00:00,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([1617, 768])\n",
      "CPU times: total: 24min 42s\n",
      "Wall time: 7min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_chunk_embeddings = get_bert_embeddings(text_chunks, batch_size=32)\n",
    "\n",
    "# Lihat shape hasil\n",
    "print(\"Embeddings shape:\", text_chunk_embeddings.shape)  # [num_sentences, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55a7f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file\n",
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48ce7e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-39</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "      <td>308</td>\n",
       "      <td>42</td>\n",
       "      <td>77.00</td>\n",
       "      <td>[-3.05841178e-01 -1.73748225e-01 -3.68292153e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-38</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>52.50</td>\n",
       "      <td>[-6.83715582e-01 -1.44871831e-01 -4.53810960e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37</td>\n",
       "      <td>Contents Preface University of Hawai‘i at Māno...</td>\n",
       "      <td>766</td>\n",
       "      <td>114</td>\n",
       "      <td>191.50</td>\n",
       "      <td>[-4.82553899e-01  3.34395736e-01 -4.54973906e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-36</td>\n",
       "      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n",
       "      <td>941</td>\n",
       "      <td>142</td>\n",
       "      <td>235.25</td>\n",
       "      <td>[-5.82269311e-01  2.03452647e-01 -5.93261480e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35</td>\n",
       "      <td>The Cardiovascular System University of Hawai‘...</td>\n",
       "      <td>998</td>\n",
       "      <td>152</td>\n",
       "      <td>249.50</td>\n",
       "      <td>[-6.98299170e-01  2.10482717e-01 -2.91191518e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
       "1          -38  Human Nutrition: 2020 Edition by University of...   \n",
       "2          -37  Contents Preface University of Hawai‘i at Māno...   \n",
       "3          -36  Lifestyles and Nutrition University of Hawai‘i...   \n",
       "4          -35  The Cardiovascular System University of Hawai‘...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               308                42              77.00   \n",
       "1               210                30              52.50   \n",
       "2               766               114             191.50   \n",
       "3               941               142             235.25   \n",
       "4               998               152             249.50   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-3.05841178e-01 -1.73748225e-01 -3.68292153e-...  \n",
       "1  [-6.83715582e-01 -1.44871831e-01 -4.53810960e-...  \n",
       "2  [-4.82553899e-01  3.34395736e-01 -4.54973906e-...  \n",
       "3  [-5.82269311e-01  2.03452647e-01 -5.93261480e-...  \n",
       "4  [-6.98299170e-01  2.10482717e-01 -2.91191518e-...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import saved file and view\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a9027",
   "metadata": {},
   "source": [
    "## 4. RAG System (use vector search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c4aaa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1680, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\n",
    "    \"embedding\"\n",
    "].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(\n",
    "    np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32\n",
    ").to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880dd3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: macronutrients functions\n",
      "\n",
      "Top results:\n",
      "1. Score: 184.0620 | Text: Table 2.1 The Eleven Organ Systems in the Human Body and Their Major Functions Organ System Organ Components Major Function Cardiovascular heart, blood/lymph vessels, blood, lymph Transport nutrients and waste products Digestive mouth, esophagus, stomach, intestines Digestion and absorption Endocrine all glands (thyroid, ovaries, pancreas) Produce and release hormones Lymphatic tonsils, adenoids, spleen and thymus A one-way system of vessels that transport lymph throughout the body Immune white blood cells, lymphatic tissue, marrow Defend against foreign invaders Integumentary skin, nails, hair, sweat glands Protective, body temperature regulation Muscular skeletal, smooth, and cardiac muscle Body movement Nervous brain, spinal cord, nerves Interprets and responds to stimuli Reproductive gonads, genitals Reproduction and sexual characteristics Respiratory lungs, nose, mouth, throat, trachea Gas exchange Skeletal bones, tendons, ligaments, joints Structure and support Urinary, Excretory kidneys, bladder, ureters Waste excretion, water balance 66 | Basic Biology, Anatomy, and Physiology\n",
      "2. Score: 183.4094 | Text: creating products that have a much longer shelf life than raw foods. Also, food processing protects the health of the consumer and allows for easier shipment and the marketing of foods by corporations. However, there are certain drawbacks. Food processing can reduce the nutritional content of raw ingredients. For example, canning involves the use of heat, which destroys the vitamin C in canned fruit. Also, certain food additives that are included during processing, such as high fructose corn syrup, can affect the health of a consumer. However, the level of added sugar can make a major difference. Small amounts of added sugar and other sweeteners, about 6 to 9 teaspoons a day or less, are not considered harmful.1 Food Additives If you examine the label for a processed food product, it is not unusual to see a long list of added materials. These natural or synthetic substances are food additives and there are more than three hundred used during food processing today. The most popular additives are benzoates, nitrites, sulfites, and sorbates, which prevent molds and yeast from growing on food.2 Food additives 1.\n",
      "3. Score: 181.7607 | Text: Participants are given a pill daily of a placebo or calcium supplement. Neither the participant nor the researcher know what group the participant is in. Considered the “gold” standard for scientific studies. Animal and Cellular Biology Studies are conducted on animals or on cells. Testing the effects of a new blood pressure drug on guinea pigs or on the lipid membrane of a cell. Less expensive than human trials. Study is not on whole humans so it may be not applicable. 42 | Types of Scientific Studies\n",
      "4. Score: 181.5826 | Text: The sodium-pota ssium pump is the primary mechanism for cells to maintain water balance between themselves and their surrounding environment . Learning Activities Technology Note: The second edition of the Human Nutrition Open Educational Resource (OER) textbook features interactive learning activities.  These activities are available in the web-based textbook and not available in the downloadable versions (EPUB, Digital PDF, Print_PDF, or Open Document). Learning activities may be used across various mobile devices, however, for the best user experience it is strongly recommended that users complete these activities using a desktop or laptop computer and in Google Chrome.   174 | Electrolytes Important for Fluid Balance\n",
      "5. Score: 181.5400 | Text: Source: Economic Research Service.http://www.ers.usda.gov/ data-products/commodity-consumption-by-population- characteristics/documentation.aspx. Learning Activities Technology Note: The second edition of the Human Nutrition Open Educational Resource (OER) textbook features interactive learning activities.  These activities are available in the web-based textbook and not available in the downloadable versions (EPUB, Digital PDF, Print_PDF, or Open Document). Health Consequences and Benefits of High-Carbohydrate Diets | 269\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# 1. Define the query\n",
    "query = \"macronutrients functions\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "\n",
    "# Tokenize query\n",
    "inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = embedding_model(**inputs)\n",
    "    query_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)  # (768,)\n",
    "\n",
    "\n",
    "# text_chunk_embeddings assumed already created, shape: [N, 768]\n",
    "# Pastikan bertipe tensor dan berada di device yang sama\n",
    "query_embedding = query_embedding.to(device)\n",
    "text_chunk_embeddings = text_chunk_embeddings.to(device)\n",
    "\n",
    "\n",
    "# Gunakan dot product (bisa juga cosine_similarity jika mau)\n",
    "dot_scores = torch.matmul(text_chunk_embeddings, query_embedding)\n",
    "\n",
    "# 4. Get top-5 most similar results\n",
    "top_k = 5\n",
    "top_scores, top_indices = torch.topk(dot_scores, k=top_k)\n",
    "\n",
    "print(\"\\nTop results:\")\n",
    "for i, (score, idx) in enumerate(zip(top_scores, top_indices)):\n",
    "    print(f\"{i+1}. Score: {score.item():.4f} | Text: {text_chunks[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "960772c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: macronutrients functions\n",
      "\n",
      "Time taken to compute cosine similarity: 0.0077 seconds\n",
      "\n",
      "Top 5 Results:\n",
      "\n",
      "Score: 0.8466\n",
      "Text:\n",
      "Enriched wheat flour refers to white flour with added vitamins.)Eat less of\n",
      "products that list HFCS and other sugars such as sucrose, honey, dextrose, and\n",
      "cane sugar in the first five ingredients. If you want to eat less processed\n",
      "foods then, in general, stay away from products with 274 | Carbohydrates and\n",
      "Personal Diet Choices\n",
      "Page number: 274\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Score: 0.8463\n",
      "Text:\n",
      "cure a disease. Science is a stepwise process that builds on past evidence and\n",
      "finally culminates into a well-accepted conclusion. Unfortunately, not all\n",
      "scientific conclusions are developed in the interest of human health, and some\n",
      "can be biased. Therefore, it is important to know where a scientific study was\n",
      "conducted and who provided the funding, as this can have an impact on the\n",
      "scientific conclusions being made. For example, an air quality study paid for by\n",
      "a tobacco company diminishes its value in the minds of readers as well as a red\n",
      "meat study performed at a laboratory funded by a national beef association.\n",
      "Nutritional Science Evolution One of the newest areas in the realm of\n",
      "nutritional science is the scientific discipline of nutritional genetics, also\n",
      "called nutrigenomics. Genes are part of DNA and contain the genetic information\n",
      "that make up all of our traits. Genes are codes for proteins and when they are\n",
      "turned “on” or “off,” they change how the body works. While we know that health\n",
      "is defined as more than just the absence of disease, there are currently very\n",
      "few accurate genetic markers of good health. Rather, there are many more genetic\n",
      "markers for disease.\n",
      "Page number: 47\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Score: 0.8328\n",
      "Text:\n",
      "If symptoms are severe a person is either treated by emergency care providers\n",
      "with an intravenous solution of glucose or given an injection of glucagon, which\n",
      "mobilizes glucose from glycogen in the liver. Some people who are not diabetic\n",
      "may experience reactive hypoglycemia. This is a condition in which people are\n",
      "sensitive to the intake of sugars, refined starches, and high GI foods.\n",
      "Individuals with reactive hypoglycemia have some symptoms of hypoglycemia.\n",
      "Symptoms are caused by a higher than normal increase in blood-insulin levels.\n",
      "This rapidly decreases blood-glucose levels to a level below what is required\n",
      "for proper brain function. Threats to Health | 1115\n",
      "Page number: 1115\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Score: 0.8318\n",
      "Text:\n",
      "http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=61 An interactive or media\n",
      "element has been excluded from this version of the text. You can view it online\n",
      "here: http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=61 An interactive or\n",
      "media element has been excluded from this version of the text. You can view it\n",
      "online here: http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=61   40 |\n",
      "Research and the Scientific Method\n",
      "Page number: 40\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Score: 0.8279\n",
      "Text:\n",
      "moving and stay healthy. You can learn more about Let’s Move! and efforts to\n",
      "encourage physical activity among adolescents at this website:\n",
      "http://www.letsmove.gov/. Learning Activities Technology Note: The second\n",
      "edition of the Human Nutrition Open Educational Resource (OER) textbook features\n",
      "interactive learning activities.  These activities are available in the web-\n",
      "based textbook and not available in the downloadable versions (EPUB, Digital\n",
      "PDF, Print_PDF, or Open Document). Learning activities may be used across\n",
      "various mobile devices, however, for the best user experience it is strongly\n",
      "recommended that users complete these activities using a desktop or laptop\n",
      "computer and in Google Chrome.   An interactive or media element has been\n",
      "excluded from this version of the text. You can view it online here:\n",
      "http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=472 Late Adolescence | 899\n",
      "Page number: 899\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Definisikan query\n",
    "from time import perf_counter as timer\n",
    "query = \"macronutrients functions\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 3. Tokenisasi dan embed query\n",
    "with torch.no_grad():\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    query_outputs = embedding_model(**query_inputs)\n",
    "    query_embedding = query_outputs.last_hidden_state[:, 0, :].squeeze(0)  # shape: (768,)\n",
    "    query_embedding = query_embedding.unsqueeze(0)  # shape: (1, 768)\n",
    "    \n",
    "\n",
    "# 4. Pastikan text_chunk_embeddings sudah tersedia\n",
    "# (Jika belum, kamu bisa panggil fungsi get_bert_embeddings() dari penjelasan sebelumnya)\n",
    "#text_chunk_embeddings = get_bert_embeddings(text_chunks)  # jika belum ada\n",
    "\n",
    "# 5. Hitung cosine similarity\n",
    "start_time = timer()\n",
    "similarity_scores = cosine_similarity(text_chunk_embeddings, query_embedding, dim=1)  # shape: (N,)\n",
    "end_time = timer()\n",
    "print(f\"\\nTime taken to compute cosine similarity: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# 6. Ambil top-k hasil\n",
    "top_k = 5\n",
    "top_scores, top_indices = torch.topk(similarity_scores, k=top_k)\n",
    "\n",
    "# 7. Fungsi bantu print teks yang terpotong rapi\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    print(textwrap.fill(text, wrap_length))\n",
    "\n",
    "# 8. Tampilkan hasil\n",
    "print(f\"\\nTop {top_k} Results:\")\n",
    "for score, idx in zip(top_scores, top_indices):\n",
    "    result = pages_and_chunks_over_min_token_len[idx]\n",
    "    print(f\"\\nScore: {score:.4f}\")\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(result[\"sentence_chunk\"])\n",
    "    print(f\"Page number: {result.get('page_number', 'N/A')}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "568af47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([168000, 768])\n"
     ]
    }
   ],
   "source": [
    "larger_embeddings = torch.randn(100 * embeddings.shape[0], 768).to(device)\n",
    "print(f\"Embeddings shape: {larger_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47f848ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'macronutrients functions'\n",
      "\n",
      "Top Results:\n",
      "Score: 0.8466\n",
      "Text:\n",
      "Enriched wheat flour refers to white flour with added vitamins.)Eat less of\n",
      "products that list HFCS and other sugars such as sucrose, honey, dextrose, and\n",
      "cane sugar in the first five ingredients. If you want to eat less processed\n",
      "foods then, in general, stay away from products with 274 | Carbohydrates and\n",
      "Personal Diet Choices\n",
      "Page number: 274\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Score: 0.8463\n",
      "Text:\n",
      "cure a disease. Science is a stepwise process that builds on past evidence and\n",
      "finally culminates into a well-accepted conclusion. Unfortunately, not all\n",
      "scientific conclusions are developed in the interest of human health, and some\n",
      "can be biased. Therefore, it is important to know where a scientific study was\n",
      "conducted and who provided the funding, as this can have an impact on the\n",
      "scientific conclusions being made. For example, an air quality study paid for by\n",
      "a tobacco company diminishes its value in the minds of readers as well as a red\n",
      "meat study performed at a laboratory funded by a national beef association.\n",
      "Nutritional Science Evolution One of the newest areas in the realm of\n",
      "nutritional science is the scientific discipline of nutritional genetics, also\n",
      "called nutrigenomics. Genes are part of DNA and contain the genetic information\n",
      "that make up all of our traits. Genes are codes for proteins and when they are\n",
      "turned “on” or “off,” they change how the body works. While we know that health\n",
      "is defined as more than just the absence of disease, there are currently very\n",
      "few accurate genetic markers of good health. Rather, there are many more genetic\n",
      "markers for disease.\n",
      "Page number: 47\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Score: 0.8328\n",
      "Text:\n",
      "If symptoms are severe a person is either treated by emergency care providers\n",
      "with an intravenous solution of glucose or given an injection of glucagon, which\n",
      "mobilizes glucose from glycogen in the liver. Some people who are not diabetic\n",
      "may experience reactive hypoglycemia. This is a condition in which people are\n",
      "sensitive to the intake of sugars, refined starches, and high GI foods.\n",
      "Individuals with reactive hypoglycemia have some symptoms of hypoglycemia.\n",
      "Symptoms are caused by a higher than normal increase in blood-insulin levels.\n",
      "This rapidly decreases blood-glucose levels to a level below what is required\n",
      "for proper brain function. Threats to Health | 1115\n",
      "Page number: 1115\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Score: 0.8318\n",
      "Text:\n",
      "http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=61 An interactive or media\n",
      "element has been excluded from this version of the text. You can view it online\n",
      "here: http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=61 An interactive or\n",
      "media element has been excluded from this version of the text. You can view it\n",
      "online here: http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=61   40 |\n",
      "Research and the Scientific Method\n",
      "Page number: 40\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Score: 0.8279\n",
      "Text:\n",
      "moving and stay healthy. You can learn more about Let’s Move! and efforts to\n",
      "encourage physical activity among adolescents at this website:\n",
      "http://www.letsmove.gov/. Learning Activities Technology Note: The second\n",
      "edition of the Human Nutrition Open Educational Resource (OER) textbook features\n",
      "interactive learning activities.  These activities are available in the web-\n",
      "based textbook and not available in the downloadable versions (EPUB, Digital\n",
      "PDF, Print_PDF, or Open Document). Learning activities may be used across\n",
      "various mobile devices, however, for the best user experience it is strongly\n",
      "recommended that users complete these activities using a desktop or laptop\n",
      "computer and in Google Chrome.   An interactive or media element has been\n",
      "excluded from this version of the text. You can view it online here:\n",
      "http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=472 Late Adolescence | 899\n",
      "Page number: 899\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Helper untuk membungkus teks panjang agar rapi saat print\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "print(\"Top Results:\")\n",
    "\n",
    "# Tampilkan top-k hasil dari pencarian dot product\n",
    "for score, idx in zip(top_scores, top_indices):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    \n",
    "    # Ambil data dari dictionary\n",
    "    chunk_data = pages_and_chunks_over_min_token_len[idx]\n",
    "    \n",
    "    print(\"Text:\")\n",
    "    print_wrapped(chunk_data[\"sentence_chunk\"])\n",
    "    \n",
    "    # Tampilkan nomor halaman jika tersedia\n",
    "    if \"page_number\" in chunk_data:\n",
    "        print(f\"Page number: {chunk_data['page_number']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29781116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'macronutrients functions'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'top_results_dot_product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Loop through zipped together scores and indicies from torch.topk\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m score, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mtop_results_dot_product\u001b[49m[\u001b[38;5;241m0\u001b[39m], top_results_dot_product[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top_results_dot_product' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4dac3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from time import perf_counter as timer\n",
    "import textwrap\n",
    "\n",
    "# Load BERT model dan tokenizer sekali saja\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "bert_model.to(\"cpu\")  # atau \"cuda\" jika tersedia\n",
    "\n",
    "def embed_query_bert(query: str) -> torch.Tensor:\n",
    "    \"\"\"Mengubah query menjadi embedding vektor menggunakan BERT [CLS] token.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = bert_model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]  # Ambil [CLS] token\n",
    "        return embedding.squeeze(0)  # Shape: (768,)\n",
    "\n",
    "def retrieve_relevant_resources_bert(\n",
    "    query: str,\n",
    "    embeddings: torch.Tensor,\n",
    "    n_resources_to_return: int = 5,\n",
    "    print_time: bool = True,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Embeds a query with BERT and returns top-k cosine similarity scores and indices from a tensor of embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed query\n",
    "    query_embedding = embed_query_bert(query).unsqueeze(0)  # (1, 768)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    start_time = timer()\n",
    "    scores = cosine_similarity(embeddings, query_embedding, dim=1)\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds.\")\n",
    "\n",
    "    top_scores, top_indices = torch.topk(scores, k=n_resources_to_return)\n",
    "    return top_scores, top_indices\n",
    "\n",
    "def print_wrapped(text: str, wrap_length: int = 80):\n",
    "    \"\"\"Membungkus teks panjang agar lebih mudah dibaca di console.\"\"\"\n",
    "    print(textwrap.fill(text, wrap_length))\n",
    "\n",
    "def print_top_results_and_scores_bert(\n",
    "    query: str,\n",
    "    embeddings: torch.Tensor,\n",
    "    pages_and_chunks: list[dict],\n",
    "    n_resources_to_return: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources using BERT and prints them out in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    scores, indices = retrieve_relevant_resources_bert(\n",
    "        query=query,\n",
    "        embeddings=embeddings,\n",
    "        n_resources_to_return=n_resources_to_return\n",
    "    )\n",
    "\n",
    "    print(f\"\\nQuery: '{query}'\\n\")\n",
    "    print(\"Results:\")\n",
    "    for score, idx in zip(scores, indices):\n",
    "        chunk = pages_and_chunks[idx]\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        print(\"Text:\")\n",
    "        print_wrapped(chunk[\"sentence_chunk\"])\n",
    "        print(f\"Page number: {chunk.get('page_number', 'N/A')}\")\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ade7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 1617 embeddings: 0.00547 seconds.\n",
      "\n",
      "Query: 'symptoms of pellagra'\n",
      "\n",
      "Results:\n",
      "Score: 0.8262\n",
      "Text:\n",
      "Learning Activities Technology Note: The second edition of the Human Nutrition\n",
      "Open Educational Resource (OER) textbook features interactive learning\n",
      "activities.  These activities are available in the web-based textbook and not\n",
      "available in the downloadable versions (EPUB, Digital PDF, Print_PDF, or Open\n",
      "Document). Learning activities may be used across various mobile devices,\n",
      "however, for the best user experience it is strongly recommended that users\n",
      "complete these activities using a desktop or laptop computer and in Google\n",
      "Chrome.   An interactive or media element has been excluded from this version of\n",
      "the text. You can view it online here: http://pressbooks.oer.hawaii.edu/\n",
      "humannutrition2/?p=144   An interactive or media element has been excluded from\n",
      "this version of the text. You can 170 | Regulation of Water Balance\n",
      "Page number: 170\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.8250\n",
      "Text:\n",
      "Fat intakes below 20% of energy intake will reduce the intake of fat-soluble\n",
      "vitamins and essential fatty acids, especially omega 3’s.4 Although protein\n",
      "accounts for only about 5% of energy expended, dietary protein is necessary to\n",
      "support metabolic reactions (that generate ATP), and to help muscles with\n",
      "maintenance, growth, and repair. During exercise, these metabolic reactions for\n",
      "generating ATP rely heavily on proteins such as enzymes and transport proteins.\n",
      "It is recommended that athletes consume 1.2 to 2.0 g/ kg/day of proteins in\n",
      "order to support these functions. Higher intakes may also be needed for short\n",
      "periods of intense training 4. Nutrition and Athletic Performance. (2016),\n",
      "American College of Sports Medicine. Medicine & Science in Sports &\n",
      "Exercise, 48(3), 543- 568. https://journals.lww.com/ acsm-\n",
      "msse/Fulltext/2016/03000/ Nutrition_and_Athletic_Performance.25.aspx. Accessed\n",
      "March 17, 2018.\n",
      "Page number: 962\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.8063\n",
      "Text:\n",
      "cure a disease. Science is a stepwise process that builds on past evidence and\n",
      "finally culminates into a well-accepted conclusion. Unfortunately, not all\n",
      "scientific conclusions are developed in the interest of human health, and some\n",
      "can be biased. Therefore, it is important to know where a scientific study was\n",
      "conducted and who provided the funding, as this can have an impact on the\n",
      "scientific conclusions being made. For example, an air quality study paid for by\n",
      "a tobacco company diminishes its value in the minds of readers as well as a red\n",
      "meat study performed at a laboratory funded by a national beef association.\n",
      "Nutritional Science Evolution One of the newest areas in the realm of\n",
      "nutritional science is the scientific discipline of nutritional genetics, also\n",
      "called nutrigenomics. Genes are part of DNA and contain the genetic information\n",
      "that make up all of our traits. Genes are codes for proteins and when they are\n",
      "turned “on” or “off,” they change how the body works. While we know that health\n",
      "is defined as more than just the absence of disease, there are currently very\n",
      "few accurate genetic markers of good health. Rather, there are many more genetic\n",
      "markers for disease.\n",
      "Page number: 47\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.8028\n",
      "Text:\n",
      "(which is the reverse of cow’s milk). Whey is much easier for infants to digest\n",
      "than casein. Casein and whey make a complete protein with all of the essential\n",
      "amino acids. Another protein in breastmilk, lactoferrin is an iron-binding\n",
      "protein  that helps keep iron away from pathogenic bacteria and facilitates the\n",
      "absorption of iron into an infant’s bloodstream. For most vitamins and minerals,\n",
      "breast milk provides adequate amounts for growth and maintenance of optimal\n",
      "health. Although the absolute amounts of some micronutrients are low, they are\n",
      "more efficiently absorbed by infants from breast milk. Other essential\n",
      "components include digestive enzymes that help a baby digest the breast milk.\n",
      "Human milk also provides the hormones and growth factors that help a newborn to\n",
      "develop. Diet and Milk Quality A mother’s health habits can impact milk\n",
      "production and quality. As during pregnancy, lactating mothers should avoid\n",
      "illegal substances and cigarettes.\n",
      "Page number: 829\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.8018\n",
      "Text:\n",
      "Your homemade shake can now replace some of the whole foods on your breakfast,\n",
      "lunch, or dinner plate. Unless you are an endurance or strength athlete and\n",
      "consume commercially sold protein bars and shakes only postexercise, these\n",
      "products are not a good dietary source of protein. Learning Activities\n",
      "Technology Note: The second edition of the Human Nutrition Open Educational\n",
      "Resource (OER) textbook 426 | Proteins, Diet, and Personal Choices\n",
      "Page number: 426\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_top_results_and_scores_bert(\n",
    "    query=\"symptoms of pellagra\",\n",
    "    embeddings=text_chunk_embeddings,  # hasil dari proses BERT sebelumnya\n",
    "    pages_and_chunks=pages_and_chunks_over_min_token_len,\n",
    "    n_resources_to_return=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706f6a4",
   "metadata": {},
   "source": [
    "## 5. Prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d35a50",
   "metadata": {},
   "source": [
    "## 6. Answer Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def export_requirements(output_file=\"requirements.txt\"):\n",
    "    try:\n",
    "        # Jalankan pip freeze dan arahkan outputnya ke file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            subprocess.check_call([\"pip\", \"freeze\"], stdout=f)\n",
    "        print(f\"Daftar paket berhasil disimpan ke {output_file}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Gagal mengekspor paket: {e}\")\n",
    "\n",
    "\n",
    "export_requirements()  # akan menyimpan ke requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_rag_gemma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
