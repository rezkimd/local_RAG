{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a688841",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation using Gemma LLMs\n",
    "\n",
    "Deskripsi :\n",
    "Projek ini ditujukan untuk mengimplementasikan Retrieval-Augmented Generation secara lokal baik sistem serta databasenya.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e468d8b",
   "metadata": {},
   "source": [
    "## Key terms\n",
    "\n",
    "| Term                                | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
    "| ----------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Token**                           | A sub-word piece of text. For example, \"hello, world!\" could be split into [\"hello\", \",\", \"world\", \"!\"]. A token can be a whole word,<br> part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.<br> Text gets broken into tokens before being passed to an LLM.                                                                                                                                                                                                                                                                                  |\n",
    "| **Embedding**                       | A learned numerical representation of a piece of data. For example, a sentence of text could be represented by a vector with<br> 768 values. Similar pieces of text (in meaning) will ideally have similar values.                                                                                                                                                                                                                                                                                                                                                                                        |\n",
    "| **Embedding model**                 | A model designed to accept input data and output a numerical representation. For example, a text embedding model may take in 384 <br>tokens of text and turn it into a vector of size 768. An embedding model can and often is different to an LLM model.                                                                                                                                                                                                                                                                                                                                                 |\n",
    "| **Similarity search/vector search** | Similarity search/vector search aims to find two vectors which are close together in high-demensional space. For example, <br>two pieces of similar text passed through an embedding model should have a high similarity score, whereas two pieces of text about<br> different topics will have a lower similarity score. Common similarity score measures are dot product and cosine similarity.                                                                                                                                                                                                         |\n",
    "| **Large Language Model (LLM)**      | A model which has been trained to numerically represent the patterns in text. A generative LLM will continue a sequence when given a sequence. <br>For example, given a sequence of the text \"hello, world!\", a genertive LLM may produce \"we're going to build a RAG pipeline today!\".<br> This generation will be highly dependant on the training data and prompt.                                                                                                                                                                                                                                     |\n",
    "| **LLM context window**              | The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default context window of 32k tokens<br> (about 96 pages of text) but can go up to 128k if needed. A recent open-source LLM from Google, Gemma (March 2024) has a context<br> window of 8,192 tokens (about 24 pages of text). A higher context window means an LLM can accept more relevant information<br> to assist with a query. For example, in a RAG pipeline, if a model has a larger context window, it can accept more reference items<br> from the retrieval system to aid with its generation.      |\n",
    "| **Prompt**                          | A common term for describing the input to a generative LLM. The idea of \"[prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)\" is to structure a text-based<br> (or potentially image-based as well) input to a generative LLM in a specific way so that the generated output is ideal. This technique is<br> possible because of a LLMs capacity for in-context learning, as in, it is able to use its representation of language to breakdown <br>the prompt and recognize what a suitable output may be (note: the output of LLMs is probable, so terms like \"may output\" are used). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c47ec",
   "metadata": {},
   "source": [
    "## Requirements and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13480dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def install_from_requirements(requirements_file=\"requirements.txt\"):\n",
    "    try:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_file]\n",
    "        )\n",
    "        print(f\"Sukses menginstal semua paket dari {requirements_file}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Gagal menginstal paket dari {requirements_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3e773",
   "metadata": {},
   "source": [
    "## 1. PDF Document Reading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9be410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'human-nutrition-text.pdf' sudah ada di 'information_file\\human-nutrition-text.pdf'.\n",
      "Pemeriksaan: File 'human-nutrition-text.pdf' ditemukan di dalam folder 'information_file'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Nama folder tujuan\n",
    "target_folder = \"information_file\"\n",
    "\n",
    "# Nama file PDF\n",
    "pdf_filename = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Path lengkap file PDF di dalam folder target\n",
    "pdf_path = os.path.join(target_folder, pdf_filename)\n",
    "\n",
    "# URL file PDF yang ingin diunduh\n",
    "url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "# Buat folder jika belum ada\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)\n",
    "    print(f\"Folder '{target_folder}' telah dibuat.\")\n",
    "\n",
    "# Download PDF jika belum ada\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"File '{pdf_filename}' tidak ditemukan di '{target_folder}'. Mengunduh...\")\n",
    "\n",
    "    try:\n",
    "        # Kirim permintaan GET ke URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Akan memunculkan HTTPError untuk respons yang buruk (status code 4xx atau 5xx)\n",
    "\n",
    "        # Buka file dalam mode binary write dan simpan konten\n",
    "        with open(pdf_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File '{pdf_filename}' telah diunduh dan disimpan di '{pdf_path}'.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gagal mengunduh file. Error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"File '{pdf_filename}' sudah ada di '{pdf_path}'.\")\n",
    "\n",
    "# Cek keberadaan file setelah (mencoba) diunduh\n",
    "if os.path.exists(pdf_path):\n",
    "    print(f\"Pemeriksaan: File '{pdf_filename}' ditemukan di dalam folder '{target_folder}'.\")\n",
    "else:\n",
    "    print(f\"Pemeriksaan: File '{pdf_filename}' TIDAK ditemukan di dalam folder '{target_folder}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b0bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1208it [00:01, 953.05it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': -41,\n",
       "  'page_char_count': 29,\n",
       "  'page_word_count': 4,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 7.25,\n",
       "  'text': 'Human Nutrition: 2020 Edition'},\n",
       " {'page_number': -40,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires !pip install PyMuPDF, see: https://github.com/pymupdf/pymupdf\n",
    "import pymupdf # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
    "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm \n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
    "\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    #doc = fitz.open(pdf_path)  # open a document\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49093be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.00</td>\n",
       "      <td>198.30</td>\n",
       "      <td>9.97</td>\n",
       "      <td>287.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.38</td>\n",
       "      <td>95.76</td>\n",
       "      <td>6.19</td>\n",
       "      <td>140.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.00</td>\n",
       "      <td>134.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>190.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1231.50</td>\n",
       "      <td>214.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>307.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1603.50</td>\n",
       "      <td>271.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>400.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>429.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>577.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count      1208.00          1208.00          1208.00                  1208.00   \n",
       "mean        562.50          1148.00           198.30                     9.97   \n",
       "std         348.86           560.38            95.76                     6.19   \n",
       "min         -41.00             0.00             1.00                     1.00   \n",
       "25%         260.75           762.00           134.00                     4.00   \n",
       "50%         562.50          1231.50           214.50                    10.00   \n",
       "75%         864.25          1603.50           271.00                    14.00   \n",
       "max        1166.00          2308.00           429.00                    32.00   \n",
       "\n",
       "       page_token_count  \n",
       "count           1208.00  \n",
       "mean             287.00  \n",
       "std              140.10  \n",
       "min                0.00  \n",
       "25%              190.50  \n",
       "50%              307.88  \n",
       "75%              400.88  \n",
       "max              577.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taking sample of text from the information file\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "random.sample(pages_and_texts, k=3)\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()\n",
    "\n",
    "#stats of the book we collected\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e8667",
   "metadata": {},
   "source": [
    "## 2. Text Splitting/Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac15b1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This another sentence.]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/ \n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a document instance as an example\n",
    "doc = nlp(\"This is a sentence. This another sentence.\")\n",
    "assert len(list(doc.sents)) == 2\n",
    "\n",
    "# Access the sentences of the document\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fa91ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [00:02<00:00, 429.41it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    \n",
    "    # Make sure all sentences are strings\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    \n",
    "    # Count the sentences \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9039df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 277,\n",
       "  'page_char_count': 1924,\n",
       "  'page_word_count': 318,\n",
       "  'page_sentence_count_raw': 16,\n",
       "  'page_token_count': 481.0,\n",
       "  'text': 'often.  • Calm your “sweet tooth” by eating fruits, such as berries or an  apple.  • Replace sugary soft drinks with seltzer water, tea, or a small  amount of 100 percent fruit juice added to water or soda water.  The Food Industry: Functional Attributes of  Carbohydrates and the Use of Sugar Substitutes  In the food industry, both fast-releasing and slow-releasing  carbohydrates are utilized to give foods a wide spectrum of  functional attributes, including increased sweetness, viscosity, bulk,  coating ability, solubility, consistency, texture, body, and browning  capacity. The differences in chemical structure between the  different carbohydrates confer their varied functional uses in foods.  Starches, gums, and pectins are used as thickening agents in making  jam, cakes, cookies, noodles, canned products, imitation cheeses,  and a variety of other foods. Molecular gastronomists use slow- releasing carbohydrates, such as alginate, to give shape and texture  to their fascinating food creations. Adding fiber to foods increases  bulk. Simple sugars are used not only for adding sweetness, but  also to add texture, consistency, and browning. In ice cream, the  combination of sucrose and corn syrup imparts sweetness as well as  a glossy appearance and smooth texture.  Due to the potential health consequences of consuming too many  added sugars, sugar substitutes have replaced them in many foods  and beverages. Sugar substitutes may be from natural sources or  artificially made. Those that are artificially made are called artificial  sweeteners and must be approved by the FDA for use in foods  and beverages. The artificial sweeteners approved by the FDA are  saccharin, aspartame, acesulfame potassium, neotame, advantame,  and sucralose. Stevia is an example of a naturally derived sugar  substitute. It comes from a plant commonly known as sugarleaf  Carbohydrates and Personal Diet Choices  |  277',\n",
       "  'sentences': ['often.',\n",
       "   ' • Calm your “sweet tooth” by eating fruits, such as berries or an  apple.',\n",
       "   ' • Replace sugary soft drinks with seltzer water, tea, or a small  amount of 100 percent fruit juice added to water or soda water.',\n",
       "   ' The Food Industry: Functional Attributes of  Carbohydrates and the Use of Sugar Substitutes  In the food industry, both fast-releasing and slow-releasing  carbohydrates are utilized to give foods a wide spectrum of  functional attributes, including increased sweetness, viscosity, bulk,  coating ability, solubility, consistency, texture, body, and browning  capacity.',\n",
       "   'The differences in chemical structure between the  different carbohydrates confer their varied functional uses in foods.',\n",
       "   ' Starches, gums, and pectins are used as thickening agents in making  jam, cakes, cookies, noodles, canned products, imitation cheeses,  and a variety of other foods.',\n",
       "   'Molecular gastronomists use slow- releasing carbohydrates, such as alginate, to give shape and texture  to their fascinating food creations.',\n",
       "   'Adding fiber to foods increases  bulk.',\n",
       "   'Simple sugars are used not only for adding sweetness, but  also to add texture, consistency, and browning.',\n",
       "   'In ice cream, the  combination of sucrose and corn syrup imparts sweetness as well as  a glossy appearance and smooth texture.',\n",
       "   ' Due to the potential health consequences of consuming too many  added sugars, sugar substitutes have replaced them in many foods  and beverages.',\n",
       "   'Sugar substitutes may be from natural sources or  artificially made.',\n",
       "   'Those that are artificially made are called artificial  sweeteners and must be approved by the FDA for use in foods  and beverages.',\n",
       "   'The artificial sweeteners approved by the FDA are  saccharin, aspartame, acesulfame potassium, neotame, advantame,  and sucralose.',\n",
       "   'Stevia is an example of a naturally derived sugar  substitute.',\n",
       "   'It comes from a plant commonly known as sugarleaf  Carbohydrates and Personal Diet Choices  |  277'],\n",
       "  'page_sentence_count_spacy': 16}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an example\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d1bd0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [00:00<00:00, 19744.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10 \n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bdd00ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 56,\n",
       "  'page_char_count': 1877,\n",
       "  'page_word_count': 334,\n",
       "  'page_sentence_count_raw': 10,\n",
       "  'page_token_count': 469.25,\n",
       "  'text': '•  Explain the anatomy and physiology of the  digestive system and other supporting organ systems  •  Describe the relationship between diet and each of  the organ systems  •  Describe the process of calculating Body Mass  Index (BMI)  The Native Hawaiians believed there was a strong connection  between health and food. Around the world, other cultures had  similar views of food and its relationship with health. A famous  quote by the Greek physician Hippocrates over two thousand years  ago, “Let food be thy medicine and medicine be thy food” bear much  relevance on our food choices and their connection to our health.  Today, the scientific community echoes Hippocrates’ statement as  it recognizes some foods as functional foods. The Academy of  Nutrition and Dietetics defines functional foods as “whole foods  and fortified, enriched, or enhanced foods that have a potentially  beneficial effect on health when consumed as part of a varied diet  on a regular basis, at effective levels.”  In the latter nineteenth century, a Russian doctor of immunology,  Elie Metchnikoff, was intrigued by the healthy life spans of people  who lived in the tribes of the northern Caucasus Mountains. What  contributed to their long lifespan and their resistance to life- threatening diseases? A possible factor lay wrapped up in a leather  satchel used to hold fermented milk. Observing the connection  between the beverage and longevity, Dr. Elie Metchnikoff began his  research on beneficial bacteria and the longevity of life that led to  his book, The Prolongation of Life. He studied the biological effects  and chemical properties of the kefir elixir whose name came from  the Turkish word “kef” or “pleasure.” To this day, kefir is one of the  most widely enjoyed beverages in Russia.  Kefir has since found its way into America, where it is marketed in  56  |  Introduction',\n",
       "  'sentences': ['•  Explain the anatomy and physiology of the  digestive system and other supporting organ systems  •  Describe the relationship between diet and each of  the organ systems  •  Describe the process of calculating Body Mass  Index (BMI)  The Native Hawaiians believed there was a strong connection  between health and food.',\n",
       "   'Around the world, other cultures had  similar views of food and its relationship with health.',\n",
       "   'A famous  quote by the Greek physician Hippocrates over two thousand years  ago, “Let food be thy medicine and medicine be thy food” bear much  relevance on our food choices and their connection to our health.',\n",
       "   ' Today, the scientific community echoes Hippocrates’ statement as  it recognizes some foods as functional foods.',\n",
       "   'The Academy of  Nutrition and Dietetics defines functional foods as “whole foods  and fortified, enriched, or enhanced foods that have a potentially  beneficial effect on health when consumed as part of a varied diet  on a regular basis, at effective levels.”',\n",
       "   ' In the latter nineteenth century, a Russian doctor of immunology,  Elie Metchnikoff, was intrigued by the healthy life spans of people  who lived in the tribes of the northern Caucasus Mountains.',\n",
       "   'What  contributed to their long lifespan and their resistance to life- threatening diseases?',\n",
       "   'A possible factor lay wrapped up in a leather  satchel used to hold fermented milk.',\n",
       "   'Observing the connection  between the beverage and longevity, Dr. Elie Metchnikoff began his  research on beneficial bacteria and the longevity of life that led to  his book, The Prolongation of Life.',\n",
       "   'He studied the biological effects  and chemical properties of the kefir elixir whose name came from  the Turkish word “kef” or “pleasure.”',\n",
       "   'To this day, kefir is one of the  most widely enjoyed beverages in Russia.',\n",
       "   ' Kefir has since found its way into America, where it is marketed in  56  |  Introduction'],\n",
       "  'page_sentence_count_spacy': 12,\n",
       "  'sentence_chunks': [['•  Explain the anatomy and physiology of the  digestive system and other supporting organ systems  •  Describe the relationship between diet and each of  the organ systems  •  Describe the process of calculating Body Mass  Index (BMI)  The Native Hawaiians believed there was a strong connection  between health and food.',\n",
       "    'Around the world, other cultures had  similar views of food and its relationship with health.',\n",
       "    'A famous  quote by the Greek physician Hippocrates over two thousand years  ago, “Let food be thy medicine and medicine be thy food” bear much  relevance on our food choices and their connection to our health.',\n",
       "    ' Today, the scientific community echoes Hippocrates’ statement as  it recognizes some foods as functional foods.',\n",
       "    'The Academy of  Nutrition and Dietetics defines functional foods as “whole foods  and fortified, enriched, or enhanced foods that have a potentially  beneficial effect on health when consumed as part of a varied diet  on a regular basis, at effective levels.”',\n",
       "    ' In the latter nineteenth century, a Russian doctor of immunology,  Elie Metchnikoff, was intrigued by the healthy life spans of people  who lived in the tribes of the northern Caucasus Mountains.',\n",
       "    'What  contributed to their long lifespan and their resistance to life- threatening diseases?',\n",
       "    'A possible factor lay wrapped up in a leather  satchel used to hold fermented milk.',\n",
       "    'Observing the connection  between the beverage and longevity, Dr. Elie Metchnikoff began his  research on beneficial bacteria and the longevity of life that led to  his book, The Prolongation of Life.',\n",
       "    'He studied the biological effects  and chemical properties of the kefir elixir whose name came from  the Turkish word “kef” or “pleasure.”'],\n",
       "   ['To this day, kefir is one of the  most widely enjoyed beverages in Russia.',\n",
       "    ' Kefir has since found its way into America, where it is marketed in  56  |  Introduction']],\n",
       "  'num_chunks': 2}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e6edd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208/1208 [00:00<00:00, 21135.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1843"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "992c8f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>583.38</td>\n",
       "      <td>734.44</td>\n",
       "      <td>112.33</td>\n",
       "      <td>183.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>347.79</td>\n",
       "      <td>447.54</td>\n",
       "      <td>71.22</td>\n",
       "      <td>111.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>280.50</td>\n",
       "      <td>315.00</td>\n",
       "      <td>44.00</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.00</td>\n",
       "      <td>746.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>186.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>890.00</td>\n",
       "      <td>1118.50</td>\n",
       "      <td>173.00</td>\n",
       "      <td>279.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>1831.00</td>\n",
       "      <td>297.00</td>\n",
       "      <td>457.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count      1843.00           1843.00           1843.00            1843.00\n",
       "mean        583.38            734.44            112.33             183.61\n",
       "std         347.79            447.54             71.22             111.89\n",
       "min         -41.00             12.00              3.00               3.00\n",
       "25%         280.50            315.00             44.00              78.75\n",
       "50%         586.00            746.00            114.00             186.50\n",
       "75%         890.00           1118.50            173.00             279.62\n",
       "max        1166.00           1831.00            297.00             457.75"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats about our chunks\n",
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2e24d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 17.5 | Text: Published August 2011. Accessed September 22, 2017. Introduction | 147\n",
      "Chunk token count: 11.75 | Text: Accessed April 15, 2018. 1046 | Comparing Diets\n",
      "Chunk token count: 4.5 | Text: 516 | Introduction\n",
      "Chunk token count: 21.0 | Text: Updated September 2003. Accessed November 28,2017. Discovering Nutrition Facts | 735\n",
      "Chunk token count: 31.5 | Text: view it online here: http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=494 944 | The Essential Elements of Physical Fitness\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_length = 40\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09610060",
   "metadata": {},
   "source": [
    "## 3. Chunk Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4049653b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Requires !pip install sentence-transformers\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      3\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-mpnet-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      4\u001b[0m                                       device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# choose the device to load the model to (note: GPU will often be *much* faster than CPU)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a list of sentences to turn into numbers\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# Requires !pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=\"cpu\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
    "\n",
    "# Create a list of sentences to turn into numbers\n",
    "sentences = [\n",
    "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
    "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
    "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
    "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
    "]\n",
    "\n",
    "# Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01cd748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "# Load Gemma 3 PT 4B model and tokenizer\n",
    "model_id = \"google/gemma-3-4b-it\"  # Gemma 3 PT 4B model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    low_cpu_mem_usage=True      # Optimize memory usage during loading\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d0d814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemma_embeddings(sentences, model, tokenizer):\n",
    "    \"\"\"Generate embeddings from Gemma model with proper handling to avoid NaN values\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize with padding and truncation for safety\n",
    "        inputs = tokenizer(\n",
    "            sentence, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512  # Limit length to avoid memory issues\n",
    "        ).to(model.device)\n",
    "        \n",
    "        try:\n",
    "            # Get hidden states with gradient tracking disabled\n",
    "            with torch.no_grad():\n",
    "                # Request hidden states explicitly\n",
    "                outputs = model(\n",
    "                    **inputs, \n",
    "                    output_hidden_states=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                \n",
    "                # Access hidden states properly (making sure it's not None)\n",
    "                if outputs.hidden_states is None:\n",
    "                    print(f\"Warning: No hidden states produced for: {sentence[:50]}...\")\n",
    "                    # Use a zero vector as fallback\n",
    "                    hidden_state = torch.zeros(1, inputs['input_ids'].shape[1], model.config.hidden_size, \n",
    "                                              device=model.device, dtype=torch.float32)\n",
    "                else:\n",
    "                    # Get last hidden layer, convert to float32 for stability\n",
    "                    hidden_state = outputs.hidden_states[-1].to(dtype=torch.float32)\n",
    "                \n",
    "                # Get attention mask and handle possible missing values\n",
    "                mask = inputs.attention_mask.unsqueeze(-1)\n",
    "                \n",
    "                # Safe mean pooling: first sum, then divide, with safety checks\n",
    "                sum_embeddings = torch.sum(hidden_state * mask, dim=1)\n",
    "                sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)  # Avoid division by zero\n",
    "                embedding = sum_embeddings / sum_mask\n",
    "                \n",
    "                # Check for NaN values and replace\n",
    "                if torch.isnan(embedding).any():\n",
    "                    print(f\"Warning: NaN detected in embedding for: {sentence[:50]}...\")\n",
    "                    # Replace NaNs with zeros\n",
    "                    embedding = torch.nan_to_num(embedding, nan=0.0)\n",
    "                \n",
    "                # Convert to numpy and add to results\n",
    "                embeddings.append(embedding.cpu().numpy().squeeze())\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence: {sentence[:50]}...\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            # Add a zero vector as fallback\n",
    "            embeddings.append(np.zeros(model.config.hidden_size))\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0505ea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaN detected in embedding for: The Sentences Transformers library provides an eas...\n",
      "Warning: NaN detected in embedding for: Sentences can be embedded one by one or as a list ...\n",
      "Warning: NaN detected in embedding for: Embeddings are one of the most powerful concepts i...\n",
      "Warning: NaN detected in embedding for: Learn to use embeddings well and you'll be well on...\n",
      "Sentence: The Sentences Transformers library provides an easy and open-source way to create embeddings.\n",
      "Embedding shape: (2560,)\n",
      "First 5 values: [0. 0. 0. 0. 0.]\n",
      "\n",
      "Sentence: Sentences can be embedded one by one or as a list of strings.\n",
      "Embedding shape: (2560,)\n",
      "First 5 values: [0. 0. 0. 0. 0.]\n",
      "\n",
      "Sentence: Embeddings are one of the most powerful concepts in machine learning!\n",
      "Embedding shape: (2560,)\n",
      "First 5 values: [0. 0. 0. 0. 0.]\n",
      "\n",
      "Sentence: Learn to use embeddings well and you'll be well on your way to being an AI engineer.\n",
      "Embedding shape: (2560,)\n",
      "First 5 values: [0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of sentences to turn into numbers\n",
    "sentences = [\n",
    "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
    "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
    "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
    "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = get_gemma_embeddings(sentences, model, tokenizer)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding shape:\", embedding.shape)\n",
    "    print(\"First 5 values:\", embedding[:5])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a9027",
   "metadata": {},
   "source": [
    "## 4. RAG System (use vector search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706f6a4",
   "metadata": {},
   "source": [
    "## 5. Prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d35a50",
   "metadata": {},
   "source": [
    "## 6. Answer Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c75a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text with Gemma\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e00851f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the concept of embeddings in NLP\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(prompt, max_length)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m      9\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m     10\u001b[0m         num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     11\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[0;32m     12\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Decode and return the generated text\u001b[39;00m\n\u001b[0;32m     16\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rezki\\anaconda3\\envs\\local_rag_gemma_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rezki\\anaconda3\\envs\\local_rag_gemma_env\\lib\\site-packages\\transformers\\generation\\utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2462\u001b[0m     )\n\u001b[0;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2466\u001b[0m         input_ids,\n\u001b[0;32m   2467\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2468\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2469\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2470\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2471\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2472\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2473\u001b[0m     )\n\u001b[0;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2482\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rezki\\anaconda3\\envs\\local_rag_gemma_env\\lib\\site-packages\\transformers\\generation\\utils.py:3476\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   3475\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[1;32m-> 3476\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompt = \"Explain the concept of embeddings in NLP\"\n",
    "response = generate_text(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daftar paket berhasil disimpan ke requirements.txt\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def export_requirements(output_file=\"requirements.txt\"):\n",
    "    try:\n",
    "        # Jalankan pip freeze dan arahkan outputnya ke file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            subprocess.check_call([\"pip\", \"freeze\"], stdout=f)\n",
    "        print(f\"Daftar paket berhasil disimpan ke {output_file}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Gagal mengekspor paket: {e}\")\n",
    "\n",
    "\n",
    "export_requirements()  # akan menyimpan ke requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_rag_gemma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
